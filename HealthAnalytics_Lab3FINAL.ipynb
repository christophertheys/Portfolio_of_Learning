{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Breast Cancer Prediction Using Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "### Sections of Lab 3:\n",
    "1. [Import Libraries](#section1)\n",
    "2. [Load the Dataset](#section2)\n",
    "3. [Preliminary Data Analysis](#section3)\n",
    "    - [Initial Data Overview](#section3.1)\n",
    "    - [Summary Statistics](#section3.2)\n",
    "4. [Exploring the Data](#section4)\n",
    "    - [Descriptive Interpretation of Data in Section 4](#section4.1)\n",
    "    - [Plotting variation](#section4.2)\n",
    "    - [Additional Recommendations based on research to optimise the descriptive interpretation](#section4.3)\n",
    "5. [Preparing the Data](#section5)\n",
    "    - [Imbalanced Dataset](#section5.1)\n",
    "6. [Feature Selection and Analysis](#section6)\n",
    "    - [Interpretation of Feature Selection and Analysis](#section6.1)\n",
    "7. [Implementing Decision Tree algorithm](#section7)\n",
    "    - [Interpretation of Decision Tree Algorithm](#section7.1)\n",
    "8. [Optimization and Hyper-parameter Tuning](#section8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section1'></a>\n",
    "## Section 1: Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer Prediction Lab 3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Loading the Dataset\n",
    "\n",
    "* The dataset is loaded into a pandas DataFrame\n",
    "\n",
    "* This allows for a preliminary view of the datasetâ€™s structure, including the names of the columns and the initial few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please enter full directory to load the .csv included in the zip file\n",
    "\n",
    "try:\n",
    "    file_path = \"breast-cancer.csv\"\n",
    "    data = pd.read_csv(file_path)\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {file_path} does not exist. Please check the file path and try again.\")\n",
    "    data = None\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"The file at {file_path} is empty. Please provide a valid data file.\")\n",
    "    data = None\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading the file: {e}\")\n",
    "    data = None\n",
    "\n",
    "if data is not None:\n",
    "    # Displaying the first five rows\n",
    "    print(data.head())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Section 3: Preliminary Data Analysis\n",
    "\n",
    "* This section aims to conduct a preliminary analysis of the dataset to ensure its readiness for further data exploration and modeling. \n",
    "\n",
    "* It involves checking the structure of the dataset, identifying and removing any duplicate rows, and ensuring that all necessary columns are present in the dataset.\n",
    "\n",
    "* This section ensures the quality of the dataset by removing duplicates and verifying the presence of all required columns, setting a solid foundation for the subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All required columns per the csv file\n",
    "columns = data.columns\n",
    "print(data.columns, \"\\n\")\n",
    "\n",
    "required_columns = [\n",
    "    'id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst'\n",
    "    ]\n",
    "\n",
    "# Duplicate rows in the dataset\n",
    "duplicate_rows = data.duplicated()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.sum()}\")\n",
    "\n",
    "# Removing any duplicate rows\n",
    "if duplicate_rows.any():\n",
    "    data = data.drop_duplicates()\n",
    "    print(\"Duplicate rows have been dropped.\")\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "\n",
    "# Checking that all columns are accounted for \n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "\n",
    "# Conditional test to ensure all columns are included for model testing\n",
    "if not missing_columns:\n",
    "    print(\"All required columns are present!\")\n",
    "    print(\"\\n\")\n",
    "else:\n",
    "    raise ValueError(f\"Missing columns: {', '.join(missing_columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "* The ValueError exception is raised if there are any missing columns in the dataset.\n",
    "\n",
    "* A detailed error message that specifies which columns are missing is generated using a formatted string that joins the names of the missing columns with a comma. This approach ensures that the script will halt execution and alert the user to the specific issue, thereby preventing silent failures and facilitating debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3.1'></a>\n",
    "### Section 3.1: Initial Data Overview\n",
    "\n",
    "* Summary of the dataset - This gives an overview of the size of your dataset.\n",
    "\n",
    "* Statistical summary for the numerical columns - Provides a statistical summary of all the numerical columns. This will give insights such as mean, standard deviation, minimum, and maximum values of each column, thus aiding in understanding the distribution and central tendency of your data.\n",
    "\n",
    "* Data Integrity - Ensuring the data integrity by checking for both duplicate rows and missing values, which are essential steps in data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the numerical columns\n",
    "print(\"Summary statistics for the numerical columns : \")\n",
    "display(data.describe())\n",
    "\n",
    "# Displaying dimensions of dataframe ***\n",
    "print(\"The dataframe has\", data.shape[0], \"rows and\", data.shape[1], \"columns, \\n\")\n",
    "\n",
    "# *** Running Test - Checking for duplicates ***\n",
    "print(\"Number of duplicate data : \",data.duplicated().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# *** Running Test - Checking for duplicates ***\n",
    "print(\"Number of missing values for each feature column : \\n\", data.isna().sum())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Exploring the Data\n",
    "\n",
    "* Exploring Descriptive Statistics and Visualizations of Individual Variables\n",
    "Kernel Density Estimate (KDE) gives a sense of the distribution of values in respective columns\n",
    "\n",
    "* Plotting Variation of All Independent Variables vs Diagnosis\n",
    "In this analysis, each numerical column in the dataset is represented through a boxplot, facilitating the identification of potential outliers as well as clarifying the central tendency and dispersion of data values within individual columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of numerical columns\n",
    "print(\"Distributions of numerical columns : \\n\")\n",
    "for column in data.select_dtypes(include=['number']).columns:\n",
    "    plt.figure()\n",
    "    sns.histplot(data[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot for numerical columns to identify potential outliers\n",
    "print(\"\\nBoxplots for numerical columns:\")\n",
    "for column in data.select_dtypes(include=['number']).columns:\n",
    "    plt.figure()\n",
    "    sns.boxplot(x=data[column])\n",
    "    plt.title(f'Boxplot of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#section4.1'></a>\n",
    "## Section 4.1: Descriptive Interpretation of Data in Section 4\n",
    "\n",
    "### Plot distributions of numerical columns\n",
    "\n",
    "* This segment of the script facilitates the visualization of value distributions for each numerical attribute in your dataset, thereby providing insights into the range, central tendency, and dispersion of values for each respective feature.\n",
    "\n",
    "### Boxplot for numerical columns to identify potential outliers\n",
    "\n",
    "* Similar to the plot distribution of numerical columns, it iterates over all numerical columns.\n",
    "\n",
    "* This section of the script enables the visualization of potential outliers and the interquartile range of each numerical column, serving as a vital tool for comprehending the data distribution and pinpointing potential data anomalies, including outliers.\n",
    "\n",
    "### Overall Interpretation\n",
    "\n",
    "* Collectively, these segments facilitate exploratory data analysis by offering visual elucidation of the distribution and attributes of the numerical variables within your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4.2'></a>\n",
    "## Section 4.2: Plotting variation of variables vs diagnosis \n",
    "\n",
    "### Interpretation :\n",
    "* Primary distinctions between malignant and benign cells lie in their radius, perimeter, and area.  \n",
    "\n",
    "* It is generally expected, given that these features often represent the size of the cells, and malignant cells tend to be larger than benign ones.\n",
    "\n",
    "* Important to note that compactness and concavity can also be distinguishing factors at times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'radius_mean', 'texture_mean', 'perimeter_mean', \n",
    "    'area_mean', 'smoothness_mean', 'compactness_mean', \n",
    "    'concavity_mean', 'concave points_mean', 'symmetry_mean', \n",
    "    'fractal_dimension_mean', 'radius_se', 'texture_se', \n",
    "    'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', \n",
    "    'concavity_se', 'concave points_se', 'symmetry_se', \n",
    "    'fractal_dimension_se', 'radius_worst', 'texture_worst', \n",
    "    'perimeter_worst', 'area_worst', 'smoothness_worst', \n",
    "    'compactness_worst', 'concavity_worst', 'concave points_worst', \n",
    "    'symmetry_worst', 'fractal_dimension_worst'\n",
    "]\n",
    "\n",
    "for column in columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data=data, x=column, hue='diagnosis', kde=True, element=\"step\", stat=\"density\", common_norm=False)\n",
    "    plt.title(f'Distribution of {column} by Diagnosis')\n",
    "    plt.xlabel(f'{column}')\n",
    "    plt.ylabel('Density')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.7)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section4.3: Additional Recommendations based on research to optimise the descriptive interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To enhance the analytical depth, it would be prudent to undertake targeted statistical examinations, including T-tests, to ascertain statistically significant disparities between malignant and benign groups across individual features, thereby substantiating the observations made.\n",
    "\n",
    "#### Code would follow these overview of steps:\n",
    "\n",
    "1. importing the following library - from scipy.stats import ttest_ind\n",
    "\n",
    "2. Grouping data by diagnosis -  The objective of segmenting the data according to the diagnosis category is to facilitate the isolated access and analysis of each group independently during the T-test and the computation of mean/standard deviation.\n",
    "\n",
    "3. Conducting a T-test for the means of two independent samples (malignant and benign groups) - Printing out the mean and standard deviation for each group to provide deeper insight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset\n",
    "prepared_data = data.copy()\n",
    "\n",
    "# Drop the 'id' column as it is not a feature\n",
    "prepared_data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# Temporarily convert the 'diagnosis' column to numerical values for correlation analysis\n",
    "diagnosis_mapping = {'M': 1, 'B': 0}\n",
    "prepared_data['diagnosis_temp'] = prepared_data['diagnosis'].replace(diagnosis_mapping)\n",
    "\n",
    "# Create a correlation matrix using the temporary numerical 'diagnosis' column\n",
    "plt.figure(figsize=[15, 13])\n",
    "corr_matrix = prepared_data.drop('diagnosis', axis=1).corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=True, fmt=\".2f\", linewidths=1, linecolor='black', cbar=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Multicollinearity Heatmap', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Drop the temporary numerical 'diagnosis' column, keeping the original 'diagnosis' column with 'M' and 'B' labels\n",
    "prepared_data.drop('diagnosis_temp', axis=1, inplace=True)\n",
    "\n",
    "# Checking the balance of the classifications in the 'diagnosis' column\n",
    "diagnosis_count = prepared_data['diagnosis'].value_counts()\n",
    "print(f\"Count of each diagnosis category:\\nMalignant (M): {diagnosis_count['M']}\\nBenign (B): {diagnosis_count['B']}\")\n",
    "\n",
    "# Visual representation of the balance in the 'diagnosis' column\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(x=diagnosis_count.index, y=diagnosis_count.values, palette='viridis')\n",
    "plt.title('Diagnosis Categories Count')\n",
    "plt.xlabel('Diagnosis')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1], labels=['Benign (B)', 'Malignant (M)'])  # Setting custom labels for the x-axis\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7, linewidth=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "* A copy of the original dataset data is created to avoid making changes directly to the original dataset, allowing for data manipulation without the loss of original data.\n",
    "\n",
    "* In this procedure, a provisional column named 'diagnosis_temp' is established, wherein 'M' is substituted with 1 and 'B' with 0, aiding in the facilitation of the correlation analysis. \n",
    "\n",
    "* Subsequently, a correlation matrix is constructed to decipher the intricate relationships between diverse features and the newly numerical 'diagnosis' column, a process visualized utilizing a heatmap to pinpoint the features most correlated with the diagnosis. Following the analytical process, the 'diagnosis_temp' column is discarded to restore the original 'diagnosis' column with its 'M' and 'B' delineations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5.1'></a>\n",
    "### Section 5.1: Imbalanced Dataset\n",
    "\n",
    "* Check Class Distribution - Confirming that stratification worked as expected\n",
    "\n",
    "* The 'plot.pie' function from the pandas library is utilized to generate pie charts, describing the respective proportions of each class within the training and testing datasets. \n",
    "This visualization distinctly exhibits the percentages, leveraging the autopct parameter to articulate the proportions explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the independent and dependent variables\n",
    "features = prepared_data.drop(columns=['diagnosis'])\n",
    "target = prepared_data['diagnosis']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "# We are using a stratified split to ensure that the train and test sets have the same proportion of class labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, stratify=target, random_state=42)\n",
    "\n",
    "# Display the number of records and features in the training and testing sets\n",
    "print(f\"Training set: {X_train.shape[0]} records, {X_train.shape[1]} features\")\n",
    "print(f\"Testing set: {X_test.shape[0]} records, {X_test.shape[1]} features\")\n",
    "\n",
    "# Getting the counts for each class in both sets\n",
    "train_counts = y_train.value_counts(normalize=True)\n",
    "test_counts = y_test.value_counts(normalize=True)\n",
    "\n",
    "# Displaying the class distribution\n",
    "print(\"\\n\")\n",
    "print(\"Class distribution in the training set:\")\n",
    "for label, value in train_counts.items():\n",
    "    print(f\"{label}: {value * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClass distribution in the testing set:\")\n",
    "for label, value in test_counts.items():\n",
    "    print(f\"{label}: {value * 100:.2f}%\")\n",
    "\n",
    "# Plotting the distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Defining labels with percentages\n",
    "train_labels = [f'{label}: {value * 100:.1f}%' for label, value in zip(train_counts.index, train_counts.values)]\n",
    "test_labels = [f'{label}: {value * 100:.1f}%' for label, value in zip(test_counts.index, test_counts.values)]\n",
    "\n",
    "# Creating pie charts\n",
    "ax[0].pie(train_counts, labels=train_labels, autopct='', shadow=True)\n",
    "ax[0].set_title('Training Set')\n",
    "\n",
    "ax[1].pie(test_counts, labels=test_labels, autopct='', shadow=True)\n",
    "ax[1].set_title('Testing Set')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Section 6: Feature Selection and Analysis\n",
    "\n",
    "* Analyzing the correlation between different features and the target variable to assist in feature selection.\n",
    "\n",
    "* Removing features with low variance.\n",
    "\n",
    "* Using univariate feature selection to find the best features based on univariate statistical tests.\n",
    "\n",
    "* Visualizing the feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, stratify=target, random_state=42)\n",
    "\n",
    "# Remove features with low variance\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "X_high_variance = sel.fit_transform(X_train)\n",
    "\n",
    "# Univariate feature selection\n",
    "X_best_features = SelectKBest(score_func=chi2, k=10)\n",
    "X_best_features.fit_transform(X_train, y_train)\n",
    "\n",
    "# Get score of each feature\n",
    "feature_scores = X_best_features.scores_\n",
    "\n",
    "# Get column names\n",
    "columns = X_train.columns\n",
    "\n",
    "# Create a dictionary and a dataframe with scores and features\n",
    "feature_dict = dict(zip(columns, feature_scores))\n",
    "feature_df = pd.DataFrame(feature_dict.items(), columns=['Feature', 'Score'])\n",
    "\n",
    "# Sort the dataframe based on score\n",
    "feature_df = feature_df.sort_values(by='Score', ascending=False)\n",
    "\n",
    "# Plot the scores\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x='Score', y='Feature', data=feature_df)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Display the feature score dataframe\n",
    "print(feature_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6.1'></a>\n",
    "### Section 6.1 - Interpretation of Feature Selection and Analysis\n",
    "\n",
    "Variance Thresholding - The selection of a threshold value set at 0.8*(1-0.8) in variance thresholding aims to eliminate features characterized by a similarity in behavior in over 80% of the samples, thereby deemed to provide restricted information for classification purposes.\n",
    "\n",
    "Univariate Feature Selection -  Implementing a parameter of k=10 facilitates the selection of the top 10 attributes grounded on the outcomes of the chi-squared statistical analysis. The chi-squared test, which assesses the independence of two events, is leveraged in this dataset to pinpoint the 10 features exhibiting the most substantial associations with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Section 7: Implementing Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the DecisionTreeClassifier with default parameters\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Step 2: Fit the model to the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions on the test data\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Step 4: Evaluate the model's performance using accuracy score, confusion matrix, and classification report\n",
    "print('Accuracy Score:', accuracy_score(y_test, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Step 5: Visualize the decision tree\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(dt_classifier, filled=True, feature_names=X_train.columns, class_names=['B', 'M'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7.1'></a>\n",
    "### Section 7.1 - Interpretation of Decision Tree Algorithm\n",
    "\n",
    "* Initialize the DecisionTreeClassifier - Setting the random_state to 42 ensures that we get the same results each time we run the script, facilitating reproducibility\n",
    "\n",
    "* Fit the Model to the Training Data - The classifier is trained utilizing the training dataset, thereby learning the underlying patterns present within the data. This knowledge equips the classifier with the ability to accurately forecast outcomes on previously unseen data.\n",
    "\n",
    "*  Predictions on the Test Data - The model uses the patterns it learned during training to predict the labels of the test data.\n",
    "\n",
    "* Decision Tree - This visualization facilitates comprehension of the decision rules employed by the classifier to formulate predictions. Within this representation, nodes depict the specific decision rules while the leaves indicate the respective outcomes, categorized as 'B' for benign and 'M' for malignant conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='section8'></a>\n",
    "## Section 8: Optimization and Hyper-parameter Tuning\n",
    "\n",
    "* Hyper-parameter Tuning: The objective is to refine the decision tree model through the identification and integration of the optimal hyperparameters, thereby augmenting the model's predictive efficacy.\n",
    "\n",
    "* Avoid Overfitting - The objective of circumventing overfitting is to avert a scenario where the model excessively adapts to the training dataset, thereby jeopardizing its ability to appropriately generalize and make predictions on unfamiliar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "}\n",
    "\n",
    "# Initialize a DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize a GridSearchCV object which will find the best hyper-parameters\n",
    "grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print('Best parameters found: ', grid_search.best_params_)\n",
    "\n",
    "# Print the best score found\n",
    "print('Best cross-validation score: {:.2f}'.format(grid_search.best_score_))\n",
    "\n",
    "# Test the model on the test data\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print('Test set score: {:.2f}'.format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of hyperparameter ranges:\n",
    "\n",
    "* Criterion -  Exploring both options to ascertain which criterion facilitates more effective tree splits, thereby enhancing the model's predictive accuracy.\n",
    "\n",
    "* Max_depth - The max_depth parameter restricts the depth of the tree. Setting it to None allows the tree to grow until it contains less than min_samples_split samples. The other values represent a series of increasing limits on the tree depth, helping us control the complexity of the model and potentially prevent overfitting\n",
    "\n",
    "* Min samples split - This parameter dictates that any node in the decision tree must contain at least 2 samples to be eligible for splitting into further nodes. A minimum value of 2 has been chosen, meaning the tree will continue splitting nodes as long as there are more than one sample in the node, thereby allowing the tree to learn finer details from the training data.\n",
    "\n",
    "* Min samples leaf - Each leaf/terminal node must contain a minimum of 5 samples from the training data. Setting this value helps in preventing the tree from creating leaves for outliers or noise in the data, thereby helping to avoid overfitting.\n",
    "\n",
    "Including a varied range of values for these hyperparameters ensures that the grid search can explore a broad space of possible models, which is essential to finding a well-tuned model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
